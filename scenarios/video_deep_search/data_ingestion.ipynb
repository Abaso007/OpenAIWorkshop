{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from pathlib import Path \n",
    "import os\n",
    "import openai\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "env_path = Path('.') / 'secrets.env'\n",
    "load_dotenv(dotenv_path=env_path)\n",
    "\n",
    "api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "azure_endpoint = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")\n",
    "import openai\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "  azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Organize video files under video folder (.mp4 files).\n",
    "Make sure each video file is in a seperate folder with the same name with video file withtout extension. For example videos/car_street1/car_street1.mp4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scenedetect import detect, AdaptiveDetector\n",
    "from PIL import Image\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import AzureOpenAI\n",
    "import base64\n",
    "client = AzureOpenAI(\n",
    "  api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),  \n",
    "  api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "  azure_endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    ")\n",
    "  \n",
    "def encode_image(image_path):  \n",
    "    with open(image_path, \"rb\") as image_file:  \n",
    "        return base64.b64encode(image_file.read()).decode('utf-8')  \n",
    "  \n",
    "def get_gpt_response(prompt, image_paths=None, max_tokens=600, json_output=False):  \n",
    "    # Encode images to base64  \n",
    "      \n",
    "    # Prepare the message content  \n",
    "    message_content = [{\"type\": \"text\", \"text\": prompt}]  \n",
    "    if image_paths:\n",
    "        base64_images = [encode_image(image_path) for image_path in image_paths]  \n",
    "\n",
    "        for base64_image in base64_images:  \n",
    "            message_content.append({  \n",
    "                \"type\": \"image_url\",  \n",
    "                \"detail\": \"low\",  \n",
    "                \"image_url\": {  \n",
    "                    \"url\": f\"data:image/jpeg;base64,{base64_image}\",  \n",
    "                },  \n",
    "            })  \n",
    "        \n",
    "    if json_output:  \n",
    "\n",
    "        response = client.chat.completions.create(  \n",
    "            model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),  \n",
    "            messages=[  \n",
    "                {  \n",
    "                    \"role\": \"user\",  \n",
    "                    \"content\": message_content,  \n",
    "                }  \n",
    "            ],  \n",
    "            max_tokens=max_tokens,  \n",
    "            response_format={ \"type\": \"json_object\" }\n",
    "            \n",
    "        )  \n",
    "    else:\n",
    "        response = client.chat.completions.create(  \n",
    "            model=os.environ.get(\"AZURE_OPENAI_CHAT_DEPLOYMENT\"),  \n",
    "            messages=[  \n",
    "                {  \n",
    "                    \"role\": \"user\",  \n",
    "                    \"content\": message_content,  \n",
    "                }  \n",
    "            ],  \n",
    "            max_tokens=max_tokens,  \n",
    "            \n",
    "        )  \n",
    "\n",
    "      \n",
    "    # Return the response content  \n",
    "    return response.choices[0].message.content  \n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os  \n",
    "import cv2  \n",
    "import json  \n",
    "import numpy as np  \n",
    "from PIL import Image  \n",
    "from scenedetect import detect, AdaptiveDetector  \n",
    "import concurrent.futures  \n",
    "  \n",
    "  \n",
    "def get_frames_from_scene(scene, video_capture, n):  \n",
    "    start_frame = scene[0].frame_num  \n",
    "    end_frame = scene[1].frame_num  \n",
    "    frame_step = (end_frame - start_frame) // (n + 1)  \n",
    "  \n",
    "    frames = []  \n",
    "    frame_ids = []  \n",
    "  \n",
    "    for i in range(1, n + 1):  \n",
    "        frame_id = start_frame + i * frame_step  \n",
    "        video_capture.set(cv2.CAP_PROP_POS_FRAMES, frame_id)  \n",
    "        _, frame = video_capture.read()  \n",
    "        frames.append(Image.fromarray(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)))  \n",
    "        frame_ids.append(frame_id)  \n",
    "  \n",
    "    return frame_ids, frames  \n",
    "  \n",
    "def save_frame_image(frame, output_path):  \n",
    "    frame.save(output_path)  \n",
    "  \n",
    "def get_frame_time(video_capture, frame_id):  \n",
    "    fps = video_capture.get(cv2.CAP_PROP_FPS)  \n",
    "    seconds = frame_id / fps  \n",
    "    return seconds  \n",
    "  \n",
    "def extract_description(image_path):  \n",
    "    return get_gpt_response(prompt=\"This is my personal photograph for evaluation. Please describe the content within the picture frame. If there are any vehicles, identify their make, type, model, and license plate if discernible. For any humans present, detail their gender, appearance, attire, and approximate age, as well as any other distinguishable features. If there are street signs, note the street names and numbers if they are legible. If any of the specified details are not visible, simply provide a general description of the visible elements.No need to add comment such as some element is not visible\", image_paths=[image_path])  \n",
    "  \n",
    "def get_frames_from_video(video_path, output_folder, n):  \n",
    "    res = []  \n",
    "    video_capture = cv2.VideoCapture(video_path)  \n",
    "    content_list = detect(video_path, AdaptiveDetector(adaptive_threshold=1.01, window_width=3))  \n",
    "    frame_to_video_mapping = {}  \n",
    "  \n",
    "    with concurrent.futures.ThreadPoolExecutor(max_workers=7) as executor:  \n",
    "        future_to_frame = {}  \n",
    "  \n",
    "        for scene_idx, scene in enumerate(content_list):  \n",
    "            frame_ids, frames = get_frames_from_scene(scene, video_capture, n)  \n",
    "            start_time = scene[0].get_timecode()  \n",
    "            end_time = scene[1].get_timecode()  \n",
    "  \n",
    "            for frame_num, (frame_id, frame) in enumerate(zip(frame_ids, frames)):  \n",
    "                frame_filename = f\"scene_{scene_idx+1}_frame_{frame_id}_{frame_num+1}.jpg\"  \n",
    "                frame_output_path = os.path.join(output_folder, frame_filename)  \n",
    "                save_frame_image(frame, frame_output_path)  \n",
    "  \n",
    "                future = executor.submit(extract_description, frame_output_path)  \n",
    "                future_to_frame[future] = frame_filename  \n",
    "  \n",
    "        for future in concurrent.futures.as_completed(future_to_frame):  \n",
    "            frame_filename = future_to_frame[future]  \n",
    "            try:  \n",
    "                description = future.result()  \n",
    "                frame_to_video_mapping[frame_filename] = {  \n",
    "                    \"video_file\": os.path.basename(video_path),  \n",
    "                    \"frame_time\": get_frame_time(video_capture, int(frame_filename.split('_')[3])),  \n",
    "                    \"description\": description  \n",
    "                }  \n",
    "            except Exception as e:  \n",
    "                print(f\"Error processing {frame_filename}: {e}\")  \n",
    "  \n",
    "    return frame_to_video_mapping  \n",
    "  \n",
    "def process_videos_in_folder(input_folder, n):  \n",
    "    frame_to_video_mapping = {}  \n",
    "  \n",
    "    for subdir, _, files in os.walk(input_folder):  \n",
    "        for file in files:  \n",
    "            if file.endswith(('.mp4', '.avi', '.mov', '.MOV', '.mkv')):  \n",
    "                video_path = os.path.join(subdir, file)  \n",
    "  \n",
    "                video_frame_mapping = get_frames_from_video(video_path, subdir, n)  \n",
    "                frame_to_video_mapping.update(video_frame_mapping)  \n",
    "      \n",
    "    json_output_path = os.path.join(\"processed_data\", 'frame_to_video_mapping.json')  \n",
    "    with open(json_output_path, 'w') as json_file:  \n",
    "        json.dump(frame_to_video_mapping, json_file, indent=4)  \n",
    "  \n",
    "input_folder = \"videos\"  # Change this to your input folder path  \n",
    "n_frames = 3  # Change this to the number of frames you want to extract from each scene  \n",
    "process_videos_in_folder(input_folder, n_frames)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ingest images data into AI Search\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Search index video_search already exists, will now recreate it\n",
      "Search index video_search created successfully\n"
     ]
    }
   ],
   "source": [
    "### Ingest images data into AI Search\n",
    "\n",
    "from openai import AzureOpenAI\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from azure.search.documents.indexes import SearchIndexClient\n",
    "from azure.search.documents.indexes.models import *\n",
    "from azure.search.documents.indexes.models import (\n",
    "    SimpleField,\n",
    "    SearchFieldDataType,\n",
    "    SearchableField,\n",
    "    SearchField,\n",
    "    VectorSearch,\n",
    "    HnswAlgorithmConfiguration,\n",
    "    VectorSearchProfile,\n",
    "    SemanticConfiguration,\n",
    "    SemanticPrioritizedFields,\n",
    "    SemanticField,\n",
    "    SemanticSearch,\n",
    "    SearchIndex,\n",
    "    \n",
    ")\n",
    "\n",
    "from pathlib import Path  \n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "def create_search_index(index_name, searchservice):\n",
    "    \n",
    "    index_client = SearchIndexClient(endpoint=searchservice,\n",
    "                                     credential=AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\")))\n",
    "    if index_name  in index_client.list_index_names():\n",
    "        print(f\"Search index {index_name} already exists, will now recreate it\")\n",
    "        index_client.delete_index(index_name)\n",
    "    \n",
    "\n",
    "    fields=[SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True),\n",
    "            SimpleField(name=\"file_name\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"frame_filename\", type=SearchFieldDataType.String),\n",
    "            SimpleField(name=\"frame_time\", type=SearchFieldDataType.Double),\n",
    "\n",
    "            SearchableField(name=\"description\", type=SearchFieldDataType.String),\n",
    "            SearchField(name=\"contentVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "            hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,\n",
    "            vector_search_dimensions=1536, vector_search_profile_name=\"myHnswProfile\"),       \n",
    "            SearchField(name=\"imageVector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single), \n",
    "            hidden=False, searchable=True, filterable=False, sortable=False, facetable=False,\n",
    "            vector_search_dimensions=1024, vector_search_profile_name=\"myHnswProfile\"),    \n",
    "            ]\n",
    "\n",
    "    index = SearchIndex(\n",
    "        name=index_name,\n",
    "        fields=fields,\n",
    "\n",
    "\n",
    "        # Create the semantic settings with the configuration\n",
    "        semantic_search = SemanticSearch(configurations=[SemanticConfiguration(\n",
    "            name=\"my-semantic-config\",\n",
    "            prioritized_fields=SemanticPrioritizedFields(\n",
    "                title_field=SemanticField(field_name=\"description\"),\n",
    "                content_fields=[SemanticField(field_name=\"description\")]\n",
    "            )\n",
    "        )]),\n",
    "            vector_search = VectorSearch(\n",
    "                algorithms=[\n",
    "                    HnswAlgorithmConfiguration(\n",
    "                        name=\"myHnsw\"\n",
    "                    )\n",
    "                ],\n",
    "                profiles=[\n",
    "                    VectorSearchProfile(\n",
    "                        name=\"myHnswProfile\",\n",
    "                        algorithm_configuration_name=\"myHnsw\",\n",
    "                    )\n",
    "                ]\n",
    "                        )\n",
    "    )\n",
    "    index_client.create_or_update_index(index)\n",
    "    print(f\"Search index {index_name} created successfully\")\n",
    "\n",
    "\n",
    "index = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "searchkey = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "openaikey = os.getenv(\"AZURE_OPENAI_API_KEY\")\n",
    "openaiservice = os.getenv(\"AZURE_OPENAI_ENDPOINT\")\n",
    "searchservice= os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "\n",
    "client = AzureOpenAI(\n",
    "api_key=openaikey,  \n",
    "api_version=os.getenv(\"AZURE_OPENAI_API_VERSION\"),\n",
    "azure_endpoint = openaiservice\n",
    ")\n",
    "\n",
    "create_search_index(index_name=index,  searchservice=searchservice)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import uuid  \n",
    "import time\n",
    "# Function to get embeddings  \n",
    "def get_text_embedding(text, model=os.getenv(\"AZURE_OPENAI_EMB_DEPLOYMENT\")):  \n",
    "    text = text.replace(\"\\n\", \" \")  \n",
    "    while True:  \n",
    "        try:  \n",
    "            embedding_response = client.embeddings.create(input = [text], model=model).data[0].embedding\n",
    "            return embedding_response  \n",
    "        except openai.error.RateLimitError:  \n",
    "            print(\"Rate limit exceeded. Retrying after 10 seconds...\")  \n",
    "            time.sleep(10)  \n",
    "\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "import requests\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv(\"secrets.env\")\n",
    "endpoint = os.getenv(\"AZURE_AI_VISION_ENDPOINT\") + \"computervision/\"\n",
    "\n",
    "def get_image_embedding(image):\n",
    "    with open(image, \"rb\") as img:\n",
    "        data = img.read()\n",
    "\n",
    "    # Vectorize Image API\n",
    "    version = \"?api-version=2024-02-01&modelVersion=latest\"\n",
    "    vectorize_img_url = endpoint + \"retrieval:vectorizeImage\" + version\n",
    "\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/octet-stream\",\n",
    "        \"Ocp-Apim-Subscription-Key\": key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(vectorize_img_url, data=data, headers=headers)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            image_vector = r.json()[\"vector\"]\n",
    "            return image_vector\n",
    "        else:\n",
    "            print(f\"An error occurred while processing {image}. Error code: {r.status_code}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "def get_text_embedding_mm(text):\n",
    "    key = os.getenv(\"AZURE_AI_VISION_API_KEY\")\n",
    "\n",
    "    # Vectorize Image API\n",
    "    version = \"?api-version=2024-02-01&model-version=2023-04-15\"\n",
    "    vectorize_img_url = endpoint + \"retrieval:vectorizeText\" + version\n",
    "    data = json.dumps({\"text\":text})\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/json\",\n",
    "        \"Ocp-Apim-Subscription-Key\": key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(vectorize_img_url, data=data, headers=headers)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            image_vector = r.json()[\"vector\"]\n",
    "            return image_vector\n",
    "        else:\n",
    "            print(f\"An error occurred while processing {text}. Error code: {r.status_code}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {text}: {e}\")\n",
    "\n",
    "    return None\n",
    "def get_image_embedding(image):\n",
    "    with open(image, \"rb\") as img:\n",
    "        data = img.read()\n",
    "    key = os.getenv(\"AZURE_AI_VISION_API_KEY\")\n",
    "    # Vectorize Image API\n",
    "    version = \"?api-version=2024-02-01&model-version=2023-04-15\"\n",
    "    vectorize_img_url = endpoint + \"retrieval:vectorizeImage\" + version\n",
    "\n",
    "    headers = {\n",
    "        \"Content-type\": \"application/octet-stream\",\n",
    "        \"Ocp-Apim-Subscription-Key\": key\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        r = requests.post(vectorize_img_url, data=data, headers=headers)\n",
    "\n",
    "        if r.status_code == 200:\n",
    "            image_vector = r.json()[\"vector\"]\n",
    "            return image_vector\n",
    "        else:\n",
    "            print(f\"An error occurred while processing {image}. Error code: {r.status_code}.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while processing {image}: {e}\")\n",
    "\n",
    "    return None\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json  \n",
    "import os  \n",
    "import uuid  \n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed  \n",
    "  \n",
    "# Assume get_text_embedding and get_image_embedding are defined elsewhere  \n",
    "# from your_module import get_text_embedding, get_image_embedding  \n",
    "  \n",
    "def process_frame(frame_filename, data):  \n",
    "    new_item = {}  \n",
    "    new_item['id'] = str(uuid.uuid4())  \n",
    "    new_item['file_name'] = data['video_file']  \n",
    "    new_item['frame_filename'] = frame_filename  \n",
    "    new_item['frame_time'] = data['frame_time']  \n",
    "    new_item['description'] = data['description']  \n",
    "  \n",
    "    # Get embeddings  \n",
    "    new_item['contentVector'] = get_text_embedding(new_item['description'])  \n",
    "    frame_image_file = os.path.join(\"videos\", new_item['file_name'].split(\".\")[0], frame_filename)  \n",
    "    new_item['imageVector'] = get_image_embedding(frame_image_file)  \n",
    "  \n",
    "    return new_item  \n",
    "  \n",
    "# Load the mapping output JSON file  \n",
    "with open(\"processed_data/frame_to_video_mapping.json\", \"r\") as f:  \n",
    "    frame_to_video_mapping = json.load(f)  \n",
    "  \n",
    "output_data = []  \n",
    "  \n",
    "# Use ThreadPoolExecutor to process frames in parallel  \n",
    "with ThreadPoolExecutor(max_workers=8) as executor:  \n",
    "    future_to_frame = {executor.submit(process_frame, frame_filename, data): frame_filename for frame_filename, data in frame_to_video_mapping.items()}  \n",
    "  \n",
    "    for future in as_completed(future_to_frame):  \n",
    "        try:  \n",
    "            new_item = future.result()  \n",
    "            output_data.append(new_item)  \n",
    "        except Exception as exc:  \n",
    "            print(f'Generated an exception: {exc}')  \n",
    "  \n",
    "# Save the processed data to a new JSON file  \n",
    "with open(\"processed_data/output_data_with_embeddings.json\", \"w\") as f:  \n",
    "    json.dump(output_data, f, indent=4)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient  \n",
    "index = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "searchkey = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "searchservice= os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "with open(\"processed_data/output_data_with_embeddings.json\", \"r\") as f:\n",
    "            loaded_output = json.load(f)  \n",
    "\n",
    "search_client = SearchClient(  \n",
    "        endpoint=searchservice,  \n",
    "        index_name=index,  \n",
    "        credential=AzureKeyCredential(searchkey)  \n",
    "    )  \n",
    "search_client.upload_documents(loaded_output)  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Query Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.search.documents import SearchClient\n",
    "from azure.search.documents.models import VectorizableTextQuery,VectorizedQuery\n",
    "\n",
    "from azure.core.credentials import AzureKeyCredential\n",
    "from IPython.display import Image\n",
    "import ast\n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.models import QueryType, QueryCaptionType, QueryAnswerType\n",
    "\n",
    "index = os.getenv(\"AZURE_SEARCH_INDEX_NAME\")\n",
    "searchkey = os.getenv(\"AZURE_SEARCH_KEY\")\n",
    "searchservice= os.getenv(\"AZURE_SEARCH_ENDPOINT\")\n",
    "\n",
    "search_client = SearchClient(  \n",
    "        endpoint=searchservice,  \n",
    "        index_name=index,  \n",
    "        credential=AzureKeyCredential(os.getenv(\"AZURE_SEARCH_KEY\"))  \n",
    "    )  \n",
    "# Pure Vector Search\n",
    "# query = \"Nescaf brand guidelines for creating a storyboard for an iced latte coffee recipe focusing on innovation\"  \n",
    "query= \"THaco truck with frontier\"\n",
    "image_query = VectorizedQuery(vector=get_text_embedding_mm(query), k_nearest_neighbors=3, fields=\"imageVector\")\n",
    "vector_query = VectorizedQuery(vector=get_text_embedding(query), k_nearest_neighbors=3, fields=\"contentVector\")\n",
    "\n",
    "  \n",
    "results = search_client.search(  \n",
    "    search_text=query,  \n",
    "    query_type=QueryType.SEMANTIC, semantic_configuration_name='my-semantic-config', query_caption=QueryCaptionType.EXTRACTIVE, query_answer=QueryAnswerType.EXTRACTIVE,\n",
    "\n",
    "    vector_queries= [image_query,vector_query],\n",
    "    select=[ \"file_name\", \"description\", \"frame_filename\"],\n",
    "    top=3\n",
    ")  \n",
    "images_directory=\"videos\" \n",
    "\n",
    "for result in results:  \n",
    "    print(f\"file_name: {result['file_name']}\")  \n",
    "    print(f\"frame_filename: {result['frame_filename']}\")  \n",
    "\n",
    "    print(f\"description: {result['description']}\")  \n",
    "\n",
    "    image = os.path.join(images_directory,result['file_name'].split(\".\")[0], result['frame_filename'])\n",
    "    display(Image(filename=image))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "genai",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
