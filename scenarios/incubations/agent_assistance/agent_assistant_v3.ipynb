{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "import openai\n",
    "import re\n",
    "import requests\n",
    "import pandas as pd\n",
    "from openai.embeddings_utils import get_embedding, cosine_similarity\n",
    "\n",
    "API_KEY = \"6e78d72911ba4064803b8b14e1f272b8\"\n",
    "RESOURCE_ENDPOINT = \"https://azopenaidemo.openai.azure.com/\" \n",
    "\n",
    "openai.api_type = \"azure\"\n",
    "openai.api_key = API_KEY\n",
    "openai.api_base = RESOURCE_ENDPOINT\n",
    "#Note: The openai-python library support for Azure OpenAI is in preview.\n",
    "openai.api_version = \"2023-05-15\"\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data generation (Skip this step if data is already generated )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_message = \"\"\n",
    "def generate_titles():\n",
    "    user_message =f\"\"\" \n",
    "    generate 100 titles of customer support articles in your industry, focusing on the areas of payroll and HR that support agent can look up to answer questions from customers who are employees with payroll managed by ADP.\n",
    "    Output data into a list \n",
    " \n",
    "\"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a customer support agent for ADP company\"},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "            \n",
    "\n",
    "titles = generate_titles()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "titles = titles.split(\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1. Understanding Your ADP Pay Statement',\n",
       " '2. How to Enroll in ADP Direct Deposit',\n",
       " '3. Updating Your Tax Information with ADP',\n",
       " '4. How to Register for ADP Employee Self Service',\n",
       " '5. What to Do When Your Paycheck is Late',\n",
       " '6. How to Access Payroll Reports in ADP',\n",
       " '7. Understanding Your ADP W-2 Form',\n",
       " '8. How to Change Your ADP Password',\n",
       " '9. Adding or Removing Dependents in ADP',\n",
       " '10. How to View Your Pay Stub in ADP']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "titles[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "def generate_article(title):\n",
    "    user_message =f\"\"\" \n",
    "    given this title of an article \"{title}\" that support agents can look up to answer questions from customers who are employees with payroll managed by ADP.\n",
    "    Generate detail content of the article \n",
    " \n",
    "\"\"\"\n",
    "    i=0\n",
    "    while i<5:\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": \"You are a customer support agent for ADP company\"},\n",
    "                    {\"role\": \"user\", \"content\":user_message },\n",
    "                ]\n",
    "            )\n",
    "            return response['choices'][0]['message']['content']\n",
    "        except:\n",
    "            i+=1\n",
    "            time.sleep(5)\n",
    "            \n",
    "\n",
    "contents =[]\n",
    "for title in titles:\n",
    "    contents.append(generate_article(title))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "folder_path =\"../../../data/agent_assistant\"\n",
    "os.makedirs(folder_path, exist_ok=True)\n",
    "i=0\n",
    "articles ={}\n",
    "for title, content in zip(titles, contents):\n",
    "    file_name = f\"article_{i}\"\n",
    "    file_content = title+\"\\n\\n\"+content\n",
    "\n",
    "    with open(os.path.join(folder_path,file_name), 'w') as file:\n",
    "        file.write(file_content)\n",
    "\n",
    "    articles[file_name]= file_content\n",
    "    i+=1\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Document Chunking Approach"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SECTION_LENGTH = 1000\n",
    "SENTENCE_SEARCH_LIMIT = 100\n",
    "SECTION_OVERLAP = 100\n",
    "verbose= True\n",
    "def split_text(all_text):\n",
    "    SENTENCE_ENDINGS = [\".\", \"!\", \"?\"]\n",
    "    WORDS_BREAKS = [\",\", \";\", \":\", \" \", \"(\", \")\", \"[\", \"]\", \"{\", \"}\", \"\\t\", \"\\n\"]\n",
    "    # if verbose: print(f\"Splitting '{filename}' into sections\")\n",
    "\n",
    "    # def find_page(offset):\n",
    "    #     l = len(page_map)\n",
    "    #     for i in range(l - 1):\n",
    "    #         if offset >= page_map[i][1] and offset < page_map[i + 1][1]:\n",
    "    #             return i\n",
    "    #     return l - 1\n",
    "\n",
    "    # all_text = \"\".join(p[2] for p in page_map)\n",
    "    length = len(all_text)\n",
    "    start = 0\n",
    "    end = length\n",
    "    chunk=0\n",
    "    while start + SECTION_OVERLAP < length:\n",
    "        last_word = -1\n",
    "        end = start + MAX_SECTION_LENGTH\n",
    "\n",
    "        if end > length:\n",
    "            end = length\n",
    "        else:\n",
    "            # Try to find the end of the sentence\n",
    "            while end < length and (end - start - MAX_SECTION_LENGTH) < SENTENCE_SEARCH_LIMIT and all_text[end] not in SENTENCE_ENDINGS:\n",
    "                if all_text[end] in WORDS_BREAKS:\n",
    "                    last_word = end\n",
    "                end += 1\n",
    "            if end < length and all_text[end] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "                end = last_word # Fall back to at least keeping a whole word\n",
    "        if end < length:\n",
    "            end += 1\n",
    "\n",
    "        # Try to find the start of the sentence or at least a whole word boundary\n",
    "        last_word = -1\n",
    "        while start > 0 and start > end - MAX_SECTION_LENGTH - 2 * SENTENCE_SEARCH_LIMIT and all_text[start] not in SENTENCE_ENDINGS:\n",
    "            if all_text[start] in WORDS_BREAKS:\n",
    "                last_word = start\n",
    "            start -= 1\n",
    "        if all_text[start] not in SENTENCE_ENDINGS and last_word > 0:\n",
    "            start = last_word\n",
    "        if start > 0:\n",
    "            start += 1\n",
    "\n",
    "        section_text = all_text[start:end]\n",
    "        chunk += 1\n",
    "        yield (section_text, chunk)\n",
    "\n",
    "        start = end - SECTION_OVERLAP\n",
    "        \n",
    "    if start + SECTION_OVERLAP < end:\n",
    "        yield (all_text[start:end], chunk)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enriching article chunks with Summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_version = \"2023-05-15\"\n",
    "# import json\n",
    "# import time\n",
    "# gpt_turbo_model = \"gpt-35-turbo\"\n",
    "# gpt_4_32k_model = \"\" #find name of gpt_4_32k model\n",
    "\n",
    "# def enrich_chunk(chunk_content):\n",
    "#     user_message=f\"\"\" \n",
    "#     Given the document below, extract the main topic and a short summary of the content\n",
    "#     {{\"topic\": \"Topic Name\", \"summary\": \"short summary of content\"}}\n",
    "#     Just output data, do not add any comment.\n",
    "#     <<document>>\n",
    "#     {chunk_content}\n",
    "#     <<document>>\n",
    "#     \"\"\"\n",
    "    \n",
    "\n",
    "#     system_message = \"\"\"\n",
    "#     You are an AI assistant that helps extract information from text. \n",
    "#     \"\"\"\n",
    "#     gpt_model= gpt_turbo_model #default choice, it can be better to go with GPT-4\n",
    "#     i=0\n",
    "#     output=\"\"\n",
    "#     while i<10: #if the output format is not as expected or if there's a throttling error, retry up to 10 times.\n",
    "#         try:\n",
    "#             response = openai.ChatCompletion.create(\n",
    "#                 engine=gpt_model, \n",
    "#                 messages=[\n",
    "#                     {\"role\": \"system\", \"content\": system_message},\n",
    "#                     {\"role\": \"user\", \"content\":user_message },\n",
    "#                 ]\n",
    "#             )\n",
    "#             response=response['choices'][0]['message']['content']\n",
    "#             output = json.loads(response)\n",
    "#             output[\"topic\"],output[\"summary\"] #this is just to validate the format\n",
    "#             break\n",
    "#         except Exception as e:\n",
    "#             print(\"temporary exception occured, will retry after 3s\", str(e))\n",
    "#             i+=1\n",
    "#             time.sleep(3)\n",
    "#     if len(\"output\")==0:\n",
    "#         raise Exception(\"Cannot extract the content after retrying 10 times\")\n",
    "#     return output\n",
    "# enriched_chunked_articles=[]\n",
    "# for article in enriched_articles:\n",
    "#     article_file = article[\"article_file\"]\n",
    "#     article_content = article[\"article_content\"]\n",
    "#     enriched_article_chunks =[]\n",
    "#     for chunk_content, chunk_number in split_text(article_content):\n",
    "#         enriched_chunk = enrich_chunk(chunk_content)\n",
    "#         enriched_chunk['chunk_content'] = chunk_content\n",
    "#         enriched_article_chunks.append(enriched_chunk)\n",
    "\n",
    "\n",
    "#     enriched_chunked_articles.append((article_file,article_content, enriched_article_chunks))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Building map for topic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# openai.api_version = \"2022-12-01\"\n",
    "# topic_content={}\n",
    "# for enriched_article in enriched_chunked_articles:\n",
    "#     chunks = enriched_article[2]\n",
    "#     for chunk in chunks:\n",
    "#         article_topic_id= enriched_article[0]+\"###\"+ chunk['topic']\n",
    "#         topic_content[article_topic_id]=chunk['chunk_content']\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple chunking without enrichment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "import json\n",
    "import time\n",
    "chunked_articles=[]\n",
    "for article in enriched_articles:\n",
    "    article_file = article[\"article_file\"]\n",
    "    article_content = article[\"article_content\"]\n",
    "    article_chunks =[]\n",
    "    for chunk_content, chunk_number in split_text(article_content):\n",
    "        article_chunks.append(chunk_content)\n",
    "    chunked_articles.append((article_file,article_content, article_chunks))\n",
    "openai.api_version = \"2022-12-01\"\n",
    "chunks_emb=[]\n",
    "for article in chunked_articles:\n",
    "    chunks = article[2]\n",
    "    chunk_no =0\n",
    "    for chunk in chunks:\n",
    "        chunk_id= article[0]+\"###\"+ str(chunk_no)\n",
    "        chunk_emb = get_embedding(chunk, engine = 'text-embedding-ada-002')\n",
    "        chunks_emb.append((chunk_id, chunk, chunk_emb))\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "\n",
    "with open(os.path.join(folder_path,\"chunk_emb_map.json\"), \"w\") as fp:\n",
    "    json.dump(chunks_emb,fp) \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Semantic Chunking and Enrichment with GPT\n",
    "To help semantic search better, we can run GPT models (ChatGPT, GPT-4, GPT-4-32k) through knowledge articles to create topics out of each document together with original content. Basically, the idea is to break the raw article content into topics with clear description so that search can better map question/query with right document\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "temporary exception occured, will retry after 3s Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
      "temporary exception occured, will retry after 3s Expecting ',' delimiter: line 3 column 459 (char 586)\n",
      "temporary exception occured, will retry after 3s Expecting ',' delimiter: line 1 column 1343 (char 1342)\n",
      "temporary exception occured, will retry after 3s Expecting property name enclosed in double quotes: line 1 column 3 (char 2)\n",
      "temporary exception occured, will retry after 3s The operation was timeout. { \"error\": { \"code\": \"Timeout\", \"message\": \"The operation was timeout.\" } } 408 {'error': {'code': 'Timeout', 'message': 'The operation was timeout.'}} {'Content-Length': '75', 'Content-Type': 'application/json', 'apim-request-id': 'a87746f9-8d52-4867-a1e1-52b8da28715e', 'Strict-Transport-Security': 'max-age=31536000; includeSubDomains; preload', 'x-content-type-options': 'nosniff', 'x-ms-region': 'East US', 'Date': 'Mon, 22 May 2023 21:28:00 GMT'}\n",
      "temporary exception occured, will retry after 3s Expecting value: line 5 column 1 (char 2356)\n",
      "temporary exception occured, will retry after 3s Expecting ',' delimiter: line 1 column 2569 (char 2568)\n"
     ]
    }
   ],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "import json\n",
    "import time\n",
    "gpt_turbo_model = \"gpt-35-turbo\"\n",
    "gpt_4_32k_model = \"\" #find name of gpt_4_32k model\n",
    "\n",
    "def enrich(article_content):\n",
    "    user_message=f\"\"\" \n",
    "    Given the document below, extract key topics, each with a short description and corresponding extracted original content from the document. \n",
    "    Output data in a list with this format:\n",
    "    [{{\"topic\": \"Topic Name\", \"description\": \"description of topic\", \"extracted_content\": \"original content extracted from the document\"}}]\n",
    "    Just output data, do not add any comment.\n",
    "    <<document>>\n",
    "    {article_content}\n",
    "    <<document>>\n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    system_message = \"\"\"\n",
    "    You are an AI assistant that helps people organize data. \n",
    "    \"\"\"\n",
    "    gpt_model= gpt_turbo_model #default choice, it can be better to go with GPT-4\n",
    "    if len(article_content)>14500: #if the count of document character > 2x the limit of GPT_turbo, go with GPT-4-32K-model\n",
    "        gpt_model= gpt_4_32k_model\n",
    "        print(f\"using gpt-4-32k model named {gpt_model} for this enrichment\")\n",
    "\n",
    "    i=0\n",
    "    output=\"\"\n",
    "    while i<10: #if the output format is not as expected or if there's a throttling error, retry up to 10 times.\n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=gpt_model, \n",
    "                messages=[\n",
    "                    {\"role\": \"system\", \"content\": system_message},\n",
    "                    {\"role\": \"user\", \"content\":user_message },\n",
    "                ]\n",
    "            )\n",
    "            response=response['choices'][0]['message']['content']\n",
    "            output = json.loads(response)\n",
    "            for content in output:\n",
    "                content[\"topic\"],content[\"description\"],content[\"extracted_content\"] #this is just to validate the format\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(\"temporary exception occured, will retry after 3s\", str(e))\n",
    "            i+=1\n",
    "            time.sleep(3)\n",
    "    if len(\"output\")==0:\n",
    "        raise Exception(\"Cannot extract the content after retrying 10 times\")\n",
    "    return output\n",
    "enriched_articles =[]\n",
    "for article_file, article_content in articles.items():\n",
    "    enriched_article ={}\n",
    "    try:\n",
    "        enriched_article[\"enriched_content\"] = enrich(article_content)\n",
    "    except Exception:\n",
    "        print(f\"Warning, document {article_file} cannot be enriched and is not included in the result, please check\")\n",
    "    enriched_article[\"article_file\"] = article_file\n",
    "    enriched_article[\"article_content\"] = article_content\n",
    "\n",
    "    enriched_articles.append(enriched_article)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Building content map for topic "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2022-12-01\"\n",
    "topic_content={}\n",
    "for enriched_article in enriched_articles:\n",
    "    topics = enriched_article['enriched_content']\n",
    "    for topic in topics:\n",
    "        article_topic_id= enriched_article['article_file']+\"###\"+ topic['topic']\n",
    "        topic_content[article_topic_id]=topic['extracted_content']\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2022-12-01\"\n",
    "enriched_emb={}\n",
    "for enriched_article in enriched_articles:\n",
    "    topics = enriched_article['enriched_content']\n",
    "    for topic in topics:\n",
    "        article_topic_id= enriched_article['article_file']+\"###\"+ topic['topic']\n",
    "        topic_emb =get_embedding(topic['topic']+\"\\n\"+topic['description'], engine = 'text-embedding-ada-002')\n",
    "        content_emb = get_embedding(topic['topic']+\"\\n\"+topic['description']+\"\\n\"+topic['extracted_content'], engine = 'text-embedding-ada-002')\n",
    "        enriched_emb[article_topic_id]=(topic_emb,content_emb)\n",
    "        "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Persist data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "\n",
    "with open(os.path.join(folder_path,\"enriched_emb.json\"), \"w\") as fp:\n",
    "    json.dump(enriched_emb,fp) \n",
    "\n",
    "with open(os.path.join(folder_path,\"enriched_articles.json\"), \"w\") as fp:\n",
    "    json.dump(enriched_articles,fp) \n",
    "with open(os.path.join(folder_path,\"topic_content.json\"), \"w\") as fp:\n",
    "    json.dump(topic_content,fp) \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assistant Design"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "folder_path =\"../../data/agent_assistant\"\n",
    "with open(os.path.join(folder_path, \"enriched_emb.json\"), \"r\") as file:\n",
    "    enriched_emb = json.load(file)\n",
    "with open(os.path.join(folder_path, \"enriched_articles.json\"), \"r\") as file:\n",
    "    enriched_articles = json.load(file)\n",
    "with open(os.path.join(folder_path, \"topic_content.json\"), \"r\") as file:\n",
    "    topic_content = json.load(file)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Tool to generate conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "def generate_conversation(streaming =False):\n",
    "    user_message =f\"\"\" \n",
    "    Generate a phone conversation between an customer's employee and an ADP support agent in the area of HR and Payroll. The employee has question and the support agent tried to identity the problems and \n",
    "    and take time to use different search tool to come up with answer to the employee.\n",
    "    Your response:\n",
    " \n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a customer support agent for ADP company. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ],\n",
    "        stream=streaming\n",
    "    )\n",
    "    return response\n",
    "            \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Tool to extract problems from conversation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-05-15\"\n",
    "\n",
    "user_message = \"\"\n",
    "def extract_problems(conversation):\n",
    "    user_message =f\"\"\" \n",
    "    Follow this on going conversation below and extract problems that each party may need help with and formulate the search query to the knowledge base search tool.\n",
    "    <<conversattion>>\n",
    "    {conversation}\n",
    "    <<conversattion>>\n",
    "    Output your response in JSON format {{\"problem\":\"summary of problem\", \"search_query\":\"content of search query\"}}\n",
    "    There can be more than 1 problem(s)\n",
    "    Output just JSON, nothing else.\n",
    "    Your response:\n",
    " \n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a senior customer support agent for ADP company. You listen to the conversation between an agent and a customer and assist the agent to resolve the problem.\n",
    "    You are given access to knowledge base search tool to find knowledge needed to find answer to questions. \n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        timeout=100,\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content']\n",
    "            \n",
    "conversation=generate_conversation()['choices'][0]['message']['content']\n",
    "problems=extract_problems(conversation)\n",
    "problems"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Tool to find article given a problem"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Brute Force method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('article_6###0', \" Missing or Late Paychecks\\n\\nAnother common issue is when an employee does not receive their paycheck on time or doesn't receive it at all. This can be due to a variety of reasons such as technical issues, incorrect bank details, or discrepancies with employee data.\\n\\nIf an employee misses their paycheck or it is late, they should reach out to their employer or HR department and provide as much information as possible. They should also confirm that their bank details are correct and notify their employer if there are any changes.\\n\\n3. Incorrect Deductions\\n\\nEmployees may also experience issues with payroll deductions, where their paycheck does not reflect the correct amount of taxes or any other deductions that were agreed on. This can result in underpaid wages and late fees.\\n\\nIf an employee realizes that they are being underpaid and has identified that it is due to incorrect deductions, they should contact their payroll administrator to resolve the issue. A payroll administrator can make the necessary adjustments to correct the error and ensure that the employee receives the correct amount of wages.\", 0.8195816012621397), ('article_0###0', '\\n\\nDeductions\\n\\nThe deductions section of your paycheck will show any money that was taken out of your paycheck before you received it. This may include health insurance premiums, retirement contributions, and any other voluntary deductions you have chosen to make. It is important to understand these deductions and their impact on your overall take-home pay.\\n\\nNet Pay\\n\\nYour net pay is the amount of money you receive after all taxes, deductions, and contributions have been taken out of your gross pay. This is the final amount that you will receive on your paycheck.\\n\\nPay Period\\n\\nYour paycheck will also show the pay period for which you are being paid. This may be a weekly, bi-weekly, or monthly pay period. Knowing your pay period will help you understand how often you are paid and when to expect your next paycheck.\\n\\nYear-to-Date Totals\\n\\nYour paycheck will include the year-to-date totals for your income, taxes, and contributions. This information will help you keep track of your earnings over the course of the year and prepare for tax season.', 0.8067568087831424), ('article_0###0', \"1. Understanding Your Paycheck: A Guide for ADP Payroll Customers \\n\\n1. Understanding Your Paycheck: A Guide for ADP Payroll Customers\\n\\nAs an ADP Payroll customer, you may receive your paycheck via direct deposit or a physical check. It is important to understand the different sections of your paycheck and what they represent. This guide will help you understand your paycheck and answer any questions you may have.\\n\\nGross Pay\\n\\nThe first section of your paycheck will show your gross pay. This is the amount of money you earned before any taxes, deductions, or contributions were taken out. Your gross pay may include regular pay, overtime pay, holiday pay, or any other additional compensation.\\n\\nTaxes\\n\\nThe next section of your paycheck will show the different taxes you are required to pay. This may include federal income tax, state income tax, Social Security tax, and Medicare tax. The amount of taxes you pay will depend on your income, your tax exemptions, and your state's tax laws.\\n\\nDeductions\\n\\nThe deductions section of your paycheck will show any money that was taken out of your \", 0.8029889187440814), ('article_7###0', ' Like federal taxes, the amount withheld will depend on your earnings and exemptions claimed on your W-4 form.\\n\\n4. Social Security and Medicare \\nThese are taxes that are paid to the federal government to fund Social Security and Medicare programs. These deductions are based on a percentage of your earnings.\\n\\n5. Retirement Savings Plan \\nIf you are enrolled in a company-sponsored retirement savings plan, such as a 401(k), your contributions will be listed on your pay stub. This deduction reduces your taxable income and helps you save for retirement.\\n\\n6. Health Insurance \\nIf you have health insurance through your employer, your premium payments may be deducted from your paycheck. The amount deducted will depend on the type of plan you have and the amount your employer pays.\\n\\n7. Other Deductions \\nYour pay stub may also include other deductions such as union dues, garnishments or wage attachments, or additional voluntary contributions to your retirement savings plan.\\n\\n8. Net Pay \\nThis is your take-home pay after all of your deductions have been made.', 0.7958210637696042), ('article_7###0', ' Other Deductions \\nYour pay stub may also include other deductions such as union dues, garnishments or wage attachments, or additional voluntary contributions to your retirement savings plan.\\n\\n8. Net Pay \\nThis is your take-home pay after all of your deductions have been made. It is the amount of money that will be deposited into your bank account or given to you in a paycheck.\\n\\nConclusion \\nUnderstanding your ADP pay stub is important for managing your finances and preparing your tax returns. By knowing how to read and interpret the various sections of your pay stub, you can better understand your earnings and deductions, and make any necessary adjustments to your payroll withholdings. If you have any questions or need further assistance, don’t hesitate to reach out to your HR department or ADP support team.', 0.7928801632403474)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['article_6###0',\n",
       "  'article_0###0',\n",
       "  'article_0###0',\n",
       "  'article_7###0',\n",
       "  'article_7###0'],\n",
       " [\" Missing or Late Paychecks\\n\\nAnother common issue is when an employee does not receive their paycheck on time or doesn't receive it at all. This can be due to a variety of reasons such as technical issues, incorrect bank details, or discrepancies with employee data.\\n\\nIf an employee misses their paycheck or it is late, they should reach out to their employer or HR department and provide as much information as possible. They should also confirm that their bank details are correct and notify their employer if there are any changes.\\n\\n3. Incorrect Deductions\\n\\nEmployees may also experience issues with payroll deductions, where their paycheck does not reflect the correct amount of taxes or any other deductions that were agreed on. This can result in underpaid wages and late fees.\\n\\nIf an employee realizes that they are being underpaid and has identified that it is due to incorrect deductions, they should contact their payroll administrator to resolve the issue. A payroll administrator can make the necessary adjustments to correct the error and ensure that the employee receives the correct amount of wages.\",\n",
       "  '\\n\\nDeductions\\n\\nThe deductions section of your paycheck will show any money that was taken out of your paycheck before you received it. This may include health insurance premiums, retirement contributions, and any other voluntary deductions you have chosen to make. It is important to understand these deductions and their impact on your overall take-home pay.\\n\\nNet Pay\\n\\nYour net pay is the amount of money you receive after all taxes, deductions, and contributions have been taken out of your gross pay. This is the final amount that you will receive on your paycheck.\\n\\nPay Period\\n\\nYour paycheck will also show the pay period for which you are being paid. This may be a weekly, bi-weekly, or monthly pay period. Knowing your pay period will help you understand how often you are paid and when to expect your next paycheck.\\n\\nYear-to-Date Totals\\n\\nYour paycheck will include the year-to-date totals for your income, taxes, and contributions. This information will help you keep track of your earnings over the course of the year and prepare for tax season.',\n",
       "  \"1. Understanding Your Paycheck: A Guide for ADP Payroll Customers \\n\\n1. Understanding Your Paycheck: A Guide for ADP Payroll Customers\\n\\nAs an ADP Payroll customer, you may receive your paycheck via direct deposit or a physical check. It is important to understand the different sections of your paycheck and what they represent. This guide will help you understand your paycheck and answer any questions you may have.\\n\\nGross Pay\\n\\nThe first section of your paycheck will show your gross pay. This is the amount of money you earned before any taxes, deductions, or contributions were taken out. Your gross pay may include regular pay, overtime pay, holiday pay, or any other additional compensation.\\n\\nTaxes\\n\\nThe next section of your paycheck will show the different taxes you are required to pay. This may include federal income tax, state income tax, Social Security tax, and Medicare tax. The amount of taxes you pay will depend on your income, your tax exemptions, and your state's tax laws.\\n\\nDeductions\\n\\nThe deductions section of your paycheck will show any money that was taken out of your \",\n",
       "  ' Like federal taxes, the amount withheld will depend on your earnings and exemptions claimed on your W-4 form.\\n\\n4. Social Security and Medicare \\nThese are taxes that are paid to the federal government to fund Social Security and Medicare programs. These deductions are based on a percentage of your earnings.\\n\\n5. Retirement Savings Plan \\nIf you are enrolled in a company-sponsored retirement savings plan, such as a 401(k), your contributions will be listed on your pay stub. This deduction reduces your taxable income and helps you save for retirement.\\n\\n6. Health Insurance \\nIf you have health insurance through your employer, your premium payments may be deducted from your paycheck. The amount deducted will depend on the type of plan you have and the amount your employer pays.\\n\\n7. Other Deductions \\nYour pay stub may also include other deductions such as union dues, garnishments or wage attachments, or additional voluntary contributions to your retirement savings plan.\\n\\n8. Net Pay \\nThis is your take-home pay after all of your deductions have been made.',\n",
       "  ' Other Deductions \\nYour pay stub may also include other deductions such as union dues, garnishments or wage attachments, or additional voluntary contributions to your retirement savings plan.\\n\\n8. Net Pay \\nThis is your take-home pay after all of your deductions have been made. It is the amount of money that will be deposited into your bank account or given to you in a paycheck.\\n\\nConclusion \\nUnderstanding your ADP pay stub is important for managing your finances and preparing your tax returns. By knowing how to read and interpret the various sections of your pay stub, you can better understand your earnings and deductions, and make any necessary adjustments to your payroll withholdings. If you have any questions or need further assistance, don’t hesitate to reach out to your HR department or ADP support team.'])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "def find_article(question, topk=5):  \n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    best_file_name = None  \n",
    "      \n",
    "    # Compute cosine similarity between input vector and each label vector\n",
    "    cosine_list=[]  \n",
    "    for chunk_id,chunk_content, vector in chunks_emb:  \n",
    "        #by default, we use embedding for the entire content of the topic (plus topic descrition).\n",
    "        # If you you want to use embedding on just topic name and description use this code cosine_sim = cosine_similarity(input_vector, vector[0])\n",
    "        cosine_sim = cosine_similarity(input_vector, vector) \n",
    "        cosine_list.append((chunk_id,chunk_content,cosine_sim ))\n",
    "    cosine_list.sort(key=lambda x:x[2],reverse=True)\n",
    "    cosine_list= cosine_list[:topk]\n",
    "    print(cosine_list)\n",
    "    cosine_list= cosine_list[:topk]\n",
    "    best_chunks =[chunk[0] for chunk in cosine_list]\n",
    "    contents = [chunk[1] for chunk in cosine_list]\n",
    "    return best_chunks, contents\n",
    "question = \"Why is my paycheck smaller than usual\"\n",
    "find_article(question)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('article_6###Incorrect Deductions', 0.816255147684242), ('article_6###Missing or Late Paychecks', 0.8110678496892233), ('article_7###Other Deductions', 0.8045249686543529), ('article_35###Incorrect Payroll Deductions', 0.8022426013113502), ('article_74###Payroll errors', 0.7941619118045048)]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(['article_6', 'article_6', 'article_7', 'article_35', 'article_74'],\n",
       " ['Employees may also experience issues with payroll deductions, where their paycheck does not reflect the correct amount of taxes or any other deductions that were agreed on.',\n",
       "  \"Another common issue is when an employee does not receive their paycheck on time or doesn't receive it at all. This can be due to a variety of reasons such as technical issues, incorrect bank details, or discrepancies with employee data.\",\n",
       "  'Your pay stub may also include other deductions such as union dues, garnishments or wage attachments, or additional voluntary contributions to your retirement savings plan.',\n",
       "  'Employees should review their pay stubs prior to signing to ensure accuracy. If there are any discrepancies in the deductions, the employee should contact their supervisor or HR manager immediately to rectify any inaccuracies.',\n",
       "  \"Incorrect payroll calculations or discrepancies with the employee's pay can cause issues, leading to employee dissatisfaction. Agents must ensure that the employee's paystub is accurate before addressing the issue. An easy fix for this is to check for discrepancies in data input, such as hours worked, overtime, or deductions, and adjust accordingly.\"])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np  \n",
    "openai.api_version = \"2022-12-01\"\n",
    "\n",
    "def find_article(question, topk=5):  \n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    best_file_name = None  \n",
    "      \n",
    "    # Compute cosine similarity between input vector and each label vector\n",
    "    cosine_list=[]  \n",
    "    for topic_id, vector in enriched_emb.items():  \n",
    "        #by default, we use embedding for the entire content of the topic (plus topic descrition).\n",
    "        # If you you want to use embedding on just topic name and description use this code cosine_sim = cosine_similarity(input_vector, vector[0])\n",
    "        cosine_sim = cosine_similarity(input_vector, vector[1]) \n",
    "        cosine_list.append((topic_id,cosine_sim ))\n",
    "    cosine_list.sort(key=lambda x:x[1],reverse=True)\n",
    "    cosine_list= cosine_list[:topk]\n",
    "    print(cosine_list)\n",
    "    best_topics = [topic[0] for topic in cosine_list]\n",
    "    article_files =[best_topic.split(\"###\")[0] for best_topic in best_topics]\n",
    "    contents = [topic_content[best_topic] for best_topic in best_topics]\n",
    "    return article_files, contents\n",
    "question = \"Why my paycheck is smaller than usual\"\n",
    "find_article(question)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vector Searh Lib (Faiss) method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.8225051  0.82222325 0.8091605  0.80686766 0.8064399 ]]\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'article_6###Incorrect Deductions'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 35\u001b[0m\n\u001b[0;32m     33\u001b[0m     \u001b[39mreturn\u001b[39;00m article_files, output_contents\n\u001b[0;32m     34\u001b[0m question \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mWhy is my paycheck  smaller than usual\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m---> 35\u001b[0m find_article_emb_vec(question,\u001b[39m5\u001b[39;49m)\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36mfind_article_emb_vec\u001b[1;34m(question, topk)\u001b[0m\n\u001b[0;32m     29\u001b[0m best_topics \u001b[39m=\u001b[39m [topic_ids[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m i[\u001b[39m0\u001b[39m]]\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(d)\n\u001b[1;32m---> 31\u001b[0m output_contents\u001b[39m=\u001b[39m[topic_content[best_topic] \u001b[39mfor\u001b[39;00m best_topic \u001b[39min\u001b[39;00m best_topics]\n\u001b[0;32m     32\u001b[0m article_files \u001b[39m=\u001b[39m[best_topic\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m###\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m best_topic \u001b[39min\u001b[39;00m best_topics]\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m article_files, output_contents\n",
      "Cell \u001b[1;32mIn[28], line 31\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m     29\u001b[0m best_topics \u001b[39m=\u001b[39m [topic_ids[idx] \u001b[39mfor\u001b[39;00m idx \u001b[39min\u001b[39;00m i[\u001b[39m0\u001b[39m]]\n\u001b[0;32m     30\u001b[0m \u001b[39mprint\u001b[39m(d)\n\u001b[1;32m---> 31\u001b[0m output_contents\u001b[39m=\u001b[39m[topic_content[best_topic] \u001b[39mfor\u001b[39;00m best_topic \u001b[39min\u001b[39;00m best_topics]\n\u001b[0;32m     32\u001b[0m article_files \u001b[39m=\u001b[39m[best_topic\u001b[39m.\u001b[39msplit(\u001b[39m\"\u001b[39m\u001b[39m###\u001b[39m\u001b[39m\"\u001b[39m)[\u001b[39m0\u001b[39m] \u001b[39mfor\u001b[39;00m best_topic \u001b[39min\u001b[39;00m best_topics]\n\u001b[0;32m     33\u001b[0m \u001b[39mreturn\u001b[39;00m article_files, output_contents\n",
      "\u001b[1;31mKeyError\u001b[0m: 'article_6###Incorrect Deductions'"
     ]
    }
   ],
   "source": [
    "import numpy as np  \n",
    "import faiss\n",
    "\n",
    "openai.api_version = \"2022-12-01\"\n",
    "#Get the array of embeddings for all articles\n",
    "topic_ids =[]\n",
    "emb_vectors = []\n",
    "for topic_id, vector in enriched_emb.items():  \n",
    "    #by default, we use embedding for the entire content of the topic (plus topic descrition).\n",
    "    # If you you want to use embedding on just topic name and description use this code cosine_sim = cosine_similarity(input_vector, vector[0])\n",
    "    topic_ids.append(topic_id)\n",
    "    emb_vectors.append(vector[1])\n",
    "emb_vectors = np.float32(emb_vectors)\n",
    "faiss.normalize_L2(emb_vectors)\n",
    "\n",
    "index = faiss.IndexFlatIP(len(vector[1])) \n",
    "index.add(emb_vectors)\n",
    "\n",
    "\n",
    "def find_article_emb_vec(question, topk=3):  \n",
    "    \"\"\"  \n",
    "    Given an input vector and a dictionary of label vectors,  \n",
    "    returns the label with the highest cosine similarity to the input vector.  \n",
    "    \"\"\"  \n",
    "    input_vector = get_embedding(question, engine = 'text-embedding-ada-002')\n",
    "    input_vector = np.float32([input_vector])\n",
    "    faiss.normalize_L2(input_vector)\n",
    "    d,i = index.search(input_vector, k=topk)\n",
    "    best_topics = [topic_ids[idx] for idx in i[0]]\n",
    "    print(d)\n",
    "    output_contents=[topic_content[best_topic] for best_topic in best_topics]\n",
    "    article_files =[best_topic.split(\"###\")[0] for best_topic in best_topics]\n",
    "    return article_files, output_contents\n",
    "question = \"Why is my paycheck  smaller than usual\"\n",
    "find_article_emb_vec(question,5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Tool to answer question from the given problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "openai.api_version = \"2023-03-15-preview\"\n",
    "\n",
    "def answer_assist(problem, search_query):\n",
    "\n",
    "    articles, contents = find_article(search_query,2)\n",
    "    articles_contents=\"\"\n",
    "    for article, content in zip(articles, contents):\n",
    "        articles_contents += f\"\"\" \n",
    "        article:{article}\n",
    "        content: {content}\n",
    "    \"\"\"\n",
    "    articles_contents = f\"\"\"\n",
    "    <<knowledge articles>>\n",
    "    {articles_contents}\n",
    "    <<knowledge articles>>\n",
    "    \"\"\"\n",
    "    user_message =f\"\"\" \n",
    "    problem:{problem}\n",
    "    {articles_contents}\n",
    "    Your response:\n",
    "\"\"\"\n",
    "    system_message = \"\"\"\n",
    "    You are a senior customer support agent for ADP company. You listen to the conversation between an agent and a customer and assist the agent to resolve the problem.\n",
    "    Given the question or problem statement and the knowledge article you have, give the answer.\n",
    "    Rely solely to the guidance from the article.If the knowlege article is not relavant to the question, say you don't know. Do not make up your answer. \n",
    "    Cite the name of the knowledge article as source for your answer.\n",
    "    Format:\n",
    "    Answer: your answer to the question\n",
    "    Sources: [source1, source2]\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        engine=\"gpt-35-turbo\", # engine = \"deployment_name\".\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": system_message},\n",
    "            {\"role\": \"user\", \"content\":user_message },\n",
    "        ]\n",
    "    )\n",
    "    return response['choices'][0]['message']['content'], articles_contents\n",
    "\n",
    "question = \"When do I receive my W2 form?\"\n",
    "answer, articles_contents = answer_assist(question,question)\n",
    "print(answer)\n",
    "print(\"---------------content-----------------\")\n",
    "\n",
    "print(articles_contents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Put tools all together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend(conversation):\n",
    "    i=0\n",
    "    while (i<5): #handle wrong format output\n",
    "        problems=extract_problems(conversation)\n",
    "        try:\n",
    "            problems=json.loads(problems)\n",
    "            print(\"problems\", problems)\n",
    "            for problem in problems:\n",
    "                answer, articles_contents = answer_assist(problem['problem'],problem['search_query'])\n",
    "                print(answer)\n",
    "                print(\"---------------content-----------------\")\n",
    "\n",
    "                print(articles_contents)\n",
    "            break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(\"problem parsing json, problems string is \", problems)\n",
    "            i+=1\n",
    "\n",
    "        \n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One time conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conversation =generate_conversation()['choices'][0]['message']['content']\n",
    "print(f\"Conversation {conversation}\")\n",
    "recommend(conversation)        \n",
    "\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Simulate a running conversation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import concurrent\n",
    "openai.api_version = \"2023-05-15\"\n",
    "import time\n",
    "conversation_pause_duration=2\n",
    "agent_assist_freq=5\n",
    "conversation_buffer =[]\n",
    "def stream_conversation(conversation_buffer, pause_duration=5):\n",
    "    responses = generate_conversation(True)\n",
    "    conversation_counter =0\n",
    "    old_conversation_counter =0\n",
    "    for response in responses:\n",
    "        content = response['choices'][0][\"delta\"].get(\"content\",\"\")\n",
    "        conversation_buffer.append(content) \n",
    "        if \"\\n\"  not in content:\n",
    "            print(content, end=\"\")\n",
    "        else:\n",
    "            conversation_counter+=1\n",
    "            if conversation_counter > old_conversation_counter+2:\n",
    "                conversation = \"\".join(conversation_buffer)\n",
    "                print(\"starting recommendation\")\n",
    "                print(\"conversation: \", conversation)\n",
    "                recommend(conversation)\n",
    "                print(\"ending recommendation\")\n",
    "                old_conversation_counter=conversation_counter\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "stream_conversation(conversation_buffer,conversation_pause_duration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "forecasting",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
